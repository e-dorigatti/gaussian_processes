{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementation of _Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels_, Bo Han et al., 2018, https://arxiv.org/abs/1804.06872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import (Conv2D, Dense, MaxPooling2D, GlobalAveragePooling2D,\n",
    "                          Dropout, BatchNormalization, Input, LeakyReLU,\n",
    "                          AveragePooling2D)\n",
    "from keras.datasets import cifar10, mnist\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr('raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noisy_labels_simmetry(labels, eps):\n",
    "    assert len(labels.shape) == 1\n",
    "    labels = labels.ravel()\n",
    "    classes = set(labels)\n",
    "    randoms = np.random.random(labels.shape)\n",
    "    \n",
    "    noisy = [\n",
    "        lbl if r < eps else np.random.choice([x for x in classes if x != lbl])\n",
    "        for lbl, r in zip(labels, randoms)\n",
    "    ]\n",
    "    \n",
    "    return to_categorical(np.array(noisy).reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))\n",
    "\n",
    "y_train_noisy = make_noisy_labels_simmetry(y_train, 0.45)\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "y_test_noisy = make_noisy_labels_simmetry(y_test, 0.45)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(4, (3, 3), padding='same', input_shape=(28, 28, 1)), LeakyReLU(0.01), BatchNormalization(),\n",
    "        Conv2D(4, (3, 3), padding='same'), LeakyReLU(0.01), BatchNormalization(),\n",
    "        Conv2D(4, (3, 3), padding='same'), LeakyReLU(0.01),\n",
    "        MaxPooling2D((2, 2), strides=2), BatchNormalization(),\n",
    "        Conv2D(8, (3, 3), padding='same'), LeakyReLU(0.01), BatchNormalization(),\n",
    "        Conv2D(8, (3, 3), padding='same'), LeakyReLU(0.01), BatchNormalization(),\n",
    "        Conv2D(8, (3, 3), padding='same'), LeakyReLU(0.01),\n",
    "        MaxPooling2D((2, 2)), BatchNormalization(),\n",
    "        Conv2D(16, (3, 3), padding='same'), LeakyReLU(0.01), BatchNormalization(),\n",
    "        Conv2D(16, (3, 3), padding='same'), LeakyReLU(0.01), BatchNormalization(),\n",
    "        Conv2D(16, (3, 3), padding='same'), LeakyReLU(0.01),\n",
    "        GlobalAveragePooling2D(), BatchNormalization(),\n",
    "        Dense(12), LeakyReLU(0.01), BatchNormalization(),\n",
    "        Dense(10, activation='softmax'),\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10: 992 / 1024 | R: 1.0000 - Avg. loss: 1.2059 - Avg. acc.: 0.0726 | Avg. val. loss: 2.4611 - Avg. val. acc.: 0.1367\n",
      "Epoch 2 / 10: 992 / 1024 | R: 0.9100 - Avg. loss: 1.1265 - Avg. acc.: 0.1121 | Avg. val. loss: 2.3970 - Avg. val. acc.: 0.1562\n",
      "Epoch 3 / 10: 992 / 1024 | R: 0.8200 - Avg. loss: 0.9445 - Avg. acc.: 0.2308 | Avg. val. loss: 2.3146 - Avg. val. acc.: 0.1914\n",
      "Epoch 4 / 10: 992 / 1024 | R: 0.7300 - Avg. loss: 0.9093 - Avg. acc.: 0.1957 | Avg. val. loss: 2.3213 - Avg. val. acc.: 0.2305\n",
      "Epoch 5 / 10: 992 / 1024 | R: 0.6400 - Avg. loss: 0.7239 - Avg. acc.: 0.2875 | Avg. val. loss: 2.2955 - Avg. val. acc.: 0.2148\n",
      "Epoch 6 / 10: 992 / 1024 | R: 0.5500 - Avg. loss: 0.6975 - Avg. acc.: 0.2778 | Avg. val. loss: 2.2952 - Avg. val. acc.: 0.2500\n",
      "Epoch 7 / 10: 992 / 1024 | R: 0.5500 - Avg. loss: 0.7003 - Avg. acc.: 0.2500 | Avg. val. loss: 2.3735 - Avg. val. acc.: 0.2188\n",
      "Epoch 8 / 10: 992 / 1024 | R: 0.5500 - Avg. loss: 0.4440 - Avg. acc.: 0.4306 | Avg. val. loss: 2.3735 - Avg. val. acc.: 0.2695\n",
      "Epoch 9 / 10: 992 / 1024 | R: 0.5500 - Avg. loss: 0.6094 - Avg. acc.: 0.3194 | Avg. val. loss: 2.3782 - Avg. val. acc.: 0.2656\n",
      "Epoch 10 / 10: 992 / 1024 | R: 0.5500 - Avg. loss: 0.5799 - Avg. acc.: 0.3472 | Avg. val. loss: 2.4453 - Avg. val. acc.: 0.2773\n"
     ]
    }
   ],
   "source": [
    "class CoTeach:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    @staticmethod\n",
    "    def samplewise_categorical_crossentropy(y_true, y_pred):\n",
    "        return -np.log(y_pred[y_true == 1] + 1e-12)\n",
    "    \n",
    "    def get_clean_instances(self, batch_x, batch_y, model, r):\n",
    "        ''' returns the 100*r % samples with the lowest loss\n",
    "        '''\n",
    "        preds = model.predict(batch_x)\n",
    "        losses = self.samplewise_categorical_crossentropy(batch_y, preds)\n",
    "        threshold = np.quantile(losses, r)\n",
    "        mask = losses < threshold\n",
    "        return batch_x[mask], batch_y[mask]\n",
    "    \n",
    "    def train_on_batch(self, epoch, batch_x, batch_y, models):\n",
    "        ''' for every model, get the clean samples and train the other models on them\n",
    "        '''\n",
    "        r = 1 - min(self.tau * epoch / self.tk, self.tau)\n",
    "\n",
    "        losses, accs = np.zeros(len(models)), np.zeros(len(models))\n",
    "        for i, m in enumerate(models):\n",
    "            bx, by = self.get_clean_instances(batch_x, batch_y, m, r)\n",
    "            \n",
    "            for j, m2 in enumerate(models):\n",
    "                if i != j:\n",
    "                    loss, acc = m2.train_on_batch(bx, by)\n",
    "                    losses[j] += loss\n",
    "                    accs[i] += acc\n",
    "        \n",
    "        return losses / len(models), accs / len(models), r\n",
    "\n",
    "    def train(self, X, y, models, validation_data=None):\n",
    "        N = X.shape[0]\n",
    "        \n",
    "        epoch_losses, epoch_accs = [], []\n",
    "        for epoch in range(self.n_epochs):\n",
    "\n",
    "            X, y = shuffle(X, y)\n",
    "            for batch_start in range(0, N ,self.batch_size):\n",
    "                losses, accs, r = self.train_on_batch(\n",
    "                    epoch,\n",
    "                    X[batch_start:batch_start+self.batch_size],\n",
    "                    y[batch_start:batch_start+self.batch_size],\n",
    "                    models\n",
    "                )\n",
    "                \n",
    "                message = 'Epoch %d / %d: %d / %d | R: %.4f - Avg. loss: %.4f - Avg. acc.: %.4f' % (\n",
    "                    epoch + 1, self.n_epochs, batch_start, N, r, np.mean(losses), np.mean(accs)\n",
    "                )\n",
    "                print('\\r' + message, end='', flush=True)  # does not work if \\r is at the end\n",
    "\n",
    "            # score on validation set\n",
    "            if not validation_data:\n",
    "                continue\n",
    "\n",
    "            x_test, y_test = validation_data\n",
    "            loss_sum = acc_sum = 0.0\n",
    "            for m in models:\n",
    "                loss, acc = m.evaluate(x_test, y_test, verbose=0)\n",
    "                loss_sum += loss\n",
    "                acc_sum += acc\n",
    "\n",
    "            print(' | Avg. val. loss: %.4f - Avg. val. acc.: %.4f' % (\n",
    "                loss_sum / len(models), acc_sum / len(models)\n",
    "            ))\n",
    "\n",
    "\n",
    "CoTeach(\n",
    "    tau=0.45,\n",
    "    tk=5,\n",
    "    batch_size=32,\n",
    "    n_epochs=10,\n",
    ").train(\n",
    "    x_train[:1024],\n",
    "    y_train_noisy[:1024],\n",
    "    models=[make_model(), make_model()],\n",
    "    validation_data=(x_test[:128], y_test_noisy[:128])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
